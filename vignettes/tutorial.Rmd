---
title: "Project 3: package302 Tutorial"
author: Vanessa Yang
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{package302 Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction
This package is for project 3 for STAT302. It contains four functions: 
<p>
* my_t.test
* my_lm
* my_knn_cv
* my_rf_cv
</p>
These models analyze data and evaluate statistical models. It also containts a dataset my_gapminder taken from the gapminder package.
<br>


You can install my package through Github by typing in the console:
```{r install, eval=FALSE}
devtools::install_github("vanessaaaay/package302")
```


To begin, we need to load the following packages and the example dataset.
```{r setup}
library(package302)
library(magrittr)
library(ggplot2)
library(dplyr)
data("my_gapminder")
data("my_penguins")
```


## Tutorials
In the tutorial section, I will demonstrate how to use each function.

### my_t_test
Function *my_t.test}* can be used to perform a one-sample t-test. It's helpful when you want to test the null hypothesis. We are going to make some t-tests here by using **lifeExp**  variable from my_gapminder dataset.
<br>
Suppose I want to test the null hypothesis in which the mean value of lifeExp is 60. So we will set *mu = 60*.
```{r mu}
mu <- 60
```
And:
$$H_0: \mu = 60$$
We also want to set the significant level to 0.05.
$$\alpha = 0.05$$

**example 1 -- two.sided t-test:**
$$H_a: \mu \neq 60$$
```{r two.sided}
my_t.test(my_gapminder$lifeExp, "two.sided", mu)
```
From the output, we can see the t statistics, degree of freedom, p-value, and the alternative hypothesis. The p-value is greater than the significant level of 0.05, so we don't reject the null hypothesis.


**example 2 -- one.sided t-test (greater)**
```{r greater}
my_t.test(my_gapminder$lifeExp, "greater", mu)
```
We don't reject the null hypothesis because the p-value is greater than the significant level of 0.05.


**example 3 -- one.sided t-test (less)**
$$H_a: \mu < 60$$
```{r less}
my_t.test(my_gapminder$lifeExp, "less", mu)
```
The p-value for "less" is less than the significant level of 0.05. Thus, we reject the null hypothesis and accept the alternative hypothesis.


### my_lm
my_lm runs a linear regression model. we will perform a linear regression using **lifeExp** as our response variable and **gdpPercap** and **continent** as explanatory variables.
<br>

```{r my_lm, message=FALSE, warning=FALSE}
my_model <- my_lm(lifeExp ~ gdpPercap + continent, data = my_gapminder)
# display my_model
my_model
```
The coefficient for gdpPercap(`r my_model$Estimate[2]`) is positive, meaning that the lifeExp increases `r my_model$Estimate[2]` as gpdPercap increases one unit.


We can also have a hythothesis test on gdpPercap coefficient. 
<br>

We first need to set a $H_0$ and a $H_a$.
$$H_0: coef = 0$$
$$H_0: coef \neq 0$$
Then, we will test it using the significant level of 0.05.
$$ \alpha = 0.05$$
We can see that the p-value is smaller than the significant value, so we can reject the null hypothesis.


Next, we want to test the fit of the model.
```{r x and y}
object <- lifeExp ~ gdpPercap + continent
# extract the model 
model <- model.frame(object, my_gapminder)
# extract x 
x <- model.matrix(object, my_gapminder)
# extract y
y <- as.matrix(model.response(model))
```

We can visualize and compare the actual value and the fitted value.
```{r}
my_lifeExp <- x %*% my_model$Estimate + my_model$Std.Error
my_df <- data.frame("actual" = my_gapminder$lifeExp, "fitted" = my_lifeExp,
                    "color" = my_gapminder$continent) 
my_df %>%
  ggplot(aes(x = fitted, y = actual, color = color)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Actual vs. Fitted Values", x = "Fitted Values", y = "Actual Values",
       color = "Continent") +
  theme(legend.justification = c("right", "top"))
```
<br>
we can see that the model fit with values in Europe and Oceania, but other continents have outliers. Therefore, it does not fit well with the data, and we may use other models.


### my_knn_cv 
my_knn_cv is for predicting values by using k-nearest-neighbor. It also uses cross-validation to train and evaluate models in order to improve accuracy. Here, we will make a prediction for class **species** using covariates **bill_length_mm**, **bill_depth_mm**, **body_mass_g**, and **flipper_length_mm**

We first get our train and cl data. (*make sure the remove NA values)
```{r}
my_penguins <- na.omit(my_penguins)
# pull out the training data
train <- my_penguins %>% select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)
# pull out the true value of class data
cl <- my_penguins %>% select(species)
colnames(cl) <- "class"
```

<br>

Now, we are going to do the predictions ten times with k_cv = 5 and k_nn values from 1 to 10. 
```{r eval=FALSE}
# create a matrix to store the output
result <- matrix(NA, nrow = 10, ncol = 2)
# loop through the result matrix
for (i in 1:10) {
  # obtain the output
  output <- my_knn_cv(train = train, cl = cl$class, k_nn = i, k_cv = 5)
  # cv misclassfication rate
  result[i, 1] <- output$cv_error
  # training misclassification rate
  result[i, 2] <- (sum(output$class != cl) / length(cl)) %>% round(4)
}
# convert it to data.frame
result <- data.frame("Number of neighbors" = c(1:10), 
                     "cv misclassification rate" = result[, 1],
                     "training misclassification rate" = result[, 2])
result
```
We want both the cv and the training misclassification rate as small as possible. 


### my_rf_cv 
my_rf_cv is a classification method by using random forest and cross-validation. Here, we are using this function to body_mass_g using covariates bill_length_mm, bill_depth_mm, and flipper_length_mm
<br>

We will do the demonstration by setting our k equal to 2, 5, and 10. For each k, we will run the functions 30 times.

```{r mse}
# create a matrix to store the output for each k 
cv_error <- matrix(NA, nrow = 90, ncol = 2)
cv_error[, 1] <- rep(c(2, 5, 10), each = 30)
# rows begin at 1
row <- 1
#Iterate through k in c(2, 5, 10):
for(k in c(2, 5, 10)) {
  #For each value of k, run function 30 times.
  for(i in 1:30) {
    # record mse
    cv_error[row, 2] <- my_rf_cv(k)
    # go to the next row 
    row <- row + 1
  }
}
```

To better visualize the estimated MSE, we will display it in boxplots.
```{r}
my_df <- data.frame("k" = cv_error[, 1], "mse" = cv_error[, 2])
my_df %>% 
  ggplot(aes(x = factor(k), y = mse, fill = factor(k))) +
  geom_boxplot() +
  labs(title = "MSE for K folds", x = "Number of Folds", y = "MSE", 
       fill = "Number of Folds")
```
Use a table to display the average CV estimate and the standard deviation of the CV estimates across 
```{r}
mse_sum <- my_df %>% 
  # group the data by the value of k
  group_by(k) %>%
  # calculate mean and sd of MSE for each k 
  summarise(mean = mean(mse), sd = sd(mse))
mse_sum
```
We can see that the mean of MSE decreases as k increases, and sd decreases when k increases.  It occurs because as the number of folds increase, we are able to train the model more times to avoid overfitting.


